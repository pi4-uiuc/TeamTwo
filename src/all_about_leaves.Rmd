---
title: "All About Leaves"
output:
  html_document: default
  html_notebook: default
---

```{r}

library(readr)
library(dplyr)
library(reshape2)

options(#betydb_key = readLines('~/.betykey', warn = FALSE),
  betydb_url = "https://betydb.org",
  betydb_api_version = 'beta')

leaf_variables <- readRDS("~/TeamTwo/data/variables.rds") %>%
  select(variable_id, description, name, units) %>%    # Only care about the important stuff
  collect()

```

And a sufficiently large table of measurements of said variables:

```{r}

leaf_traits_bare <- readRDS('~/TeamTwo/data/traits.rds') %>%
  inner_join(leaf_variables, by = "variable_id") %>%
  select(name, mean, units, description, date, specie_id, site_id, treatment_id)

```

Let's hook leaf_traits up to the species table:

```{r}

species <- read.csv('~/TeamTwo/data/species.csv') %>%
  mutate(specie_id = id)

leaf_traits <- leaf_traits_bare %>%
  left_join(species, by = "specie_id") %>%
  select(name, mean, units, description, date, specie_id, scientificname, commonname, site_id, treatment_id)

```

Now we're cooking with gas. Some of the species and variables are rather obscure, though, so let's see which species have had the most unique variables measured, and which variables have been measured across the widest range of species:

```{r}

traits_ranking <- leaf_traits %>%
  group_by(name) %>%
  mutate(species_with_trait = length(unique(scientificname))) %>%
  select(name, species_with_trait) %>%
  unique

important_variables <- traits_ranking %>%
  filter(species_with_trait >= 20) %>%
  select(name) %>%
  collect()

important_variables = as.vector(important_variables$name)

important_leaf_traits <- leaf_traits %>%
  filter(name %in% important_variables) %>%
  select(scientificname, name, mean)

species_ranking <- important_leaf_traits %>%
  group_by(scientificname) %>%
  mutate(traits_of_species = length(unique(name))) %>%
  select(scientificname, traits_of_species) %>%
  unique

important_species <- species_ranking %>%
  filter(traits_of_species >= 4) %>%
  select(scientificname) %>%
  collect()

important_species = as.vector(important_species$scientificname)

important_leaf_traits = important_leaf_traits %>%
  filter(scientificname %in% important_species)
  
```

It appears there are only 14 leaf variables that have been measured across 20+ species, and only 17 species have had 8+ unique leaf variables measured. Concerning ourselves only with these important species and variables, why not build a program that can predict which species a plant is, given one or more measurements of its leaves?

Given a particular species, the data collected in the leaf_traits file gives a mean measurement for a particular trait recorded at some particular site and time. We find the mean of these mean measurements (we call this the supermean), and assign this value as the trait measurement for the species.

We will split our data into training data (90%) and testing data (10%) after randomizing it by row.

```{r}

important_leaf_traits = important_leaf_traits[sample(nrow(important_leaf_traits)),]

training = round(nrow(important_leaf_traits)*0.9)
testing = nrow(important_leaf_traits) - training


important_leaf_traits_training <- important_leaf_traits %>%
  head(training) %>%
  group_by(scientificname, name) %>%
  mutate(supermean = mean(mean)) %>%
  mutate(std = sd(mean)) %>%
  select(scientificname, name, supermean, std) %>%
  unique

important_leaf_traits_testing <- important_leaf_traits %>%
  tail(testing)

```

We use the following distance function to measure how close a user-provided input of trait data corresponds to the known data from the species table: if $s=(s_1, \ldots, s_n)$ is species data and $p=(p_1, \ldots, p_n)$ is measurement data, where $n$ is the number of traits compared, then \[d(s,p)=\frac{1}{n}\sum_{i=1}^{n}\left(\frac{s_i-p_i}{\sigma_i^s}\right)^2,\]


where $\sigma_i^s$ is the standard deviation of the $i^{\mathrm{th}}$ variables for the species $s$. We will only compare traits for which the species data and the measument data are both not NA. The goal is to find a species $s$ which minimizes this function.

The predictor defined below uses the above distance function to predict the species which corresponds best to a given set of plant traits data. It returns its top three guesses, ordered by minimizing the distance function $d$.

```{r}

mean_analysis <- dcast(important_leaf_traits_training, scientificname ~ name, value.var="supermean")
std_analysis <- dcast(important_leaf_traits_training, scientificname ~ name, value.var="std")

important_variables = tail(colnames(mean_analysis), -1)

for(i in 1:nrow(mean_analysis)){
  for(j in 2:ncol(mean_analysis)){
    if(is.na(std_analysis[i,j])){
      mean_analysis[i,j] = NA
    }
  }
}

species_distance <- function(species_data, species_std, data_point) {

  incomparables = 0
  n = length(species_data)
  sum_distance = 0
  for(i in 1:length(species_data)){
    if(!is.na(species_data[i]) & !is.na(data_point[i])){
      sum_distance = sum_distance + ((species_data[i]-data_point[i])/species_std[i])^2
    }
    else {
      incomparables = incomparables + 1
    }
  }
  if(incomparables >= n-1){
    return(NA)
  }
  else {
    return(sum_distance / (n - incomparables))
  }
}

predict_species <- function(c2n_leaf = NA, leaf_biomass_area = NA, leaf_biomass_plant = NA, leaf_longevity = NA, leaf_respiration_rate_m2 = NA, leaf_turnover_rate = NA, leafC = NA, leafN = NA, leafP = NA, prediction_key = NA) {
  if(missing(prediction_key)){
    prediction_key = c(c2n_leaf, leaf_biomass_area, leaf_biomass_plant, leaf_longevity, leaf_respiration_rate_m2, leaf_turnover_rate, leafC, leafN, leafP)
  }

  n = nrow(mean_analysis)
  m = ncol(mean_analysis)
  distance_column = data.frame(matrix(ncol = 1, nrow = n))
  for(i in 1:nrow(mean_analysis)){

    distance_column[i,1] = species_distance(mean_analysis[i,2:m], std_analysis[i,2:m], prediction_key)

  }
  results_table = mean_analysis %>%
    cbind(distance_column)
  results_table = results_table[,c(1,m+1)]
  colnames(results_table) <- c("Species","Difference")

  return(arrange(results_table, results_table[,2]))
}

```

Now that we have our predictor, we can see how well it does on the testing data. We'll do this by first constructing a function which takes a species name and an integer $n$, and returns how accurate the predictor's guess was given $n$ random leaf measurements from that species (as taken from the testing data).

```{r}

to_key <- function(df, cols) {
  m = length(cols)
  n = nrow(df)
  endDF = data.frame(matrix(nrow = 1, ncol = m))
  colnames(endDF) = cols
  for(col in cols){
    endDF[1,col] = NA
    for(i in 1:n){
      if(df[i,1] == col){
        endDF[1,col] = df[i,2]
      }
    }
  }
  return(endDF)
}

test_predictor <- function(species, traitsN) {

  take_from = important_leaf_traits_testing %>%
    filter(scientificname == species) %>%
    select(name, mean)
  take_from = take_from[sample(nrow(take_from)),]
  take_from = take_from[!duplicated(take_from$name),]
  if(nrow(take_from) < traitsN) {
    return(NA)
  }
  else {
    # print(take_from[1:traitsN,])
    return(predict_species(prediction_key = to_key(take_from[1:traitsN,], important_variables)))
  }
}

analyze_performance <- function(traitsN) {
  analyzable_species = important_leaf_traits_testing %>%
    group_by(scientificname) %>%
    mutate(avail_traits = length(unique(name))) %>%
    filter(avail_traits >= traitsN) %>%
    select(scientificname)
  
  analyzable_species = unique(as.vector(analyzable_species$scientificname))
  endDF = data.frame(matrix(nrow = length(analyzable_species), ncol = 2))
  
  for(i in 1:length(analyzable_species)){
    endDF[i,1] = analyzable_species[i]
    endDF[i,2] = which(test_predictor(analyzable_species[i], traitsN)$Species == analyzable_species[i])
  }
  
  colnames(endDF) = c("Species","Prediction place")
  return(endDF)
}

analyze_performance(3)

# Example of how wildly some entries are distributed. The first column represents three random traits taken from Acer rubrum; the second and third columns represent the mean and standard deviation, respectively, of the traits for Acer rubrum as analyzed in the training data.

#           Sample    Acer rubrum (mean)    Acer rubrum (stdv)
#   SLA      16.6          16.0                  4.28              ~0.14σ off
# c2n_leaf   23.1          43.2                  20.1              ~1.00σ off
# leaf_resp  3.53          0.40                  0.14              ~22.4σ off !!!

# Can you blame the predictor for not thinking the sample belonged to Acer rubrum, with such a crazy leaf_resp value?
# Problems with a predictor of this sort seem unavoidable with sparse, highly variable data like this. 


```
